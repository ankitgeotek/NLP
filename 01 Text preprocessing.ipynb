{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"A:\\Work Docs\\Data Analyst work\\Campus X\\00 Datasets For coding\\IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower casing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][3].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower casing All rows of pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']  = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. <br /><br />the...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing HTML Tags\n",
    "using pythons regular expression aka-regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regular Expression Library\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    pattern = re.compile(\"<.*?>\")\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_html_tags(df['review'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. the filming tec...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1= 'Use this link to connect http://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews  and to drop a mail use https://mail.google.com/mail/u/0/#inbox'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile( r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Use this link to connect   and to drop a mail use '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Punctuation\n",
    "\n",
    "!@#$%^&*()_+-={}[]:\";'~`,.`\n",
    "\n",
    "Hello! How are you?  here punctuation are ! and ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace( char, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_punc = \"String. with. Punctuation?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'String with Punctuation'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punc(text_punc) # it is slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String with Punctuation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(remove_punc(text_punc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is faster as compare to previous\n",
    "def remove_punc_fast(text):\n",
    "    return text.translate(str.maketrans('', '', exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'String with Punctuation'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punc_fast(text_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_punc = pd.read_csv(r\"A:\\Work Docs\\Data Analyst work\\Campus X\\00 Datasets For coding\\hate speech twitter\\train_E6oV3lV.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         user when a father is dysfunctional and is so...\n",
       "1        user user thanks for lyft credit i cant use ca...\n",
       "2                                      bihday your majesty\n",
       "3        model   i love u take with u all the time in u...\n",
       "4                     factsguide society now    motivation\n",
       "                               ...                        \n",
       "31957    ate user isz that youuuðððððð...\n",
       "31958      to see nina turner on the airwaves trying to...\n",
       "31959    listening to sad songs on a monday morning otw...\n",
       "31960    user sikh temple vandalised in in calgary wso ...\n",
       "31961                      thank you user for you follow  \n",
       "Name: tweet, Length: 31962, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_punc['tweet'].apply(remove_punc_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Words Treatment\n",
    "eg- lol, rofl, lmoa, fyi, asap, gn\n",
    "\n",
    "# Internet slang and abbreviations #\n",
    "\n",
    "Slang     | Variations      | In use?    | Meaning\n",
    ":---------|:----------------|:----------:|:-------\n",
    "`\\|`&nbsp;&nbsp;`\\|\\|`<br>`\\|\\|`&nbsp;`\\|_` | `\\| \\|\\| \\|\\| \\|_` | Yes | [Loss](https://knowyourmeme.com/news/heres-to-loss-the-internets-greatest-meme)\n",
    "/s        |                 | Yes        | Sarcasm (suffix)\n",
    "3cool5u   | 3cool5me        | Yes        | \"Too cool for you\"; from \"2cool4u\"\n",
    "420       |                 | Yes        | \"Marijuana\"\n",
    "afaik     |                 | Yes        | \"As far as I know\"\n",
    "afk       |                 | Yes        | \"Away from keyboard\"\n",
    "asl       | a/s/l           | Yes        | \"Age/sex/location?\"; sexual context\n",
    "atm       |                 | Yes        | \"At the moment\"\n",
    "atw       |                 | Rare       | \"All the way\"; expresses approval\n",
    "ayy       | ayy lmao        | Yes        | \"Yeah\"; often accompanies humor/thanks\n",
    "bae       |                 | Yes        | Informal/sarcastic form of \"baby\"\n",
    "bb        |                 | Yes        | \"Baby\" / \"Be back\"\n",
    "bbiab     |                 | Rare       | \"Be back in a bit\"\n",
    "bbl       |                 | Yes        | \"Be back later\"\n",
    "bbs       |                 | Yes        | \"Be back soon\"\n",
    "bc        | b/c, cuz        | Yes        | \"Because\"\n",
    "bf        |                 | Yes        | \"Boyfriend\"\n",
    "bff       |                 | Yes        | \"Best friend forever\"\n",
    "bork      | borked          | Yes        | \"Break\" / \"Broken\"\n",
    "brb       |                 | Yes        | \"Be right back\"\n",
    "btw       |                 | Yes        | \"By the way\"\n",
    "cba       |                 | Yes        | \"Can't be arsed\"\n",
    "convo     |                 | Yes        | \"Conversation\"\n",
    "cp        |                 | Yes        | \"Child porn\" (internet taboo)\n",
    "cya       |                 | Yes        | \"See ya [later]\"\n",
    "cya       | cu              | Yes        | \"See you [later]\"\n",
    "dank      |                 | Yes        | Similar to \"cool\"\n",
    "dc        | d/c, dc'd       | Yes        | \"Disconnect(ed)\" / \"Don't care\"\n",
    "dem feels |                 | No         | Reaction to emotional text\n",
    "dw        |                 | Yes        | \"Don't worry\"\n",
    "e2e       | e2ee            | Yes        | \"End-to-end [encryption]\"\n",
    "fml       |                 | Yes        | \"Fuck my life\" (jokingly)\n",
    "FOMO      |                 | Yes        | \"Fear of missing out\"\n",
    "FTFY      |                 | Yes        | \"Fixed that for you\" (sarcastic)\n",
    "ftl       |                 | Yes        | \"For the lose\"; expresses dislike\n",
    "ftw       |                 | Yes        | \"For the win\"; expresses approval\n",
    "fwiw      |                 | Yes        | \"For what it's worth\"\n",
    "fyi       |                 | Yes        | \"For your information\"\n",
    "g2g       | gtg             | Yes        | \"Got to go\"\n",
    "g4u       |                 | Yes        | \"Good for you\"; often sarcastic\n",
    "gf        |                 | Yes        | \"Girlfriend\"\n",
    "gg        |                 | Yes        | \"Good game\"; occasionally \"got to go\"\n",
    "goml      |                 | Yes        | \"Get on my level\"\n",
    "gr8       |                 | No         | \"Great\"\n",
    "gratz     | congratz        | Yes        | \"Congratulations\"\n",
    "gtfo      |                 | Yes        | \"Get the fuck out\"\n",
    "guiz      |                 | Yes        | \"Guys\"; meant to sound stupid\n",
    "hbu       |                 | Yes        | \"How 'bout you?\"\n",
    "hru       |                 | Yes        | \"How are you?\"\n",
    "ianadb    |                 | Yes        | \"I am not a doctor, but...\"\n",
    "ianalb    |                 | Yes        | \"I am not a lawyer, but...\"\n",
    "ianap     |                 | Yes        | \"I am not a photographer\"\n",
    "idc       |                 | Yes        | \"I don't care\"\n",
    "idgaf     |                 | Yes        | \"I don't give a fuck\"\n",
    "idk       | idfk, idek      | Yes        | \"I don't (fucking/even) know\"\n",
    "iirc      |                 | Yes        | \"If I recall correctly\"\n",
    "ik        |                 | Yes        | \"I know\"\n",
    "ikr       | inorite         | Yes        | \"I know, right?\"\n",
    "ily       | ilu             | Yes        | \"I love you\"\n",
    "inb4      |                 | Yes        | \"In before\"; prefixes a prediction\n",
    "irl       |                 | Yes        | \"In real life\"\n",
    "jfc       |                 | Yes        | \"Jesus fucking Christ\"\n",
    "jk        |                 | Yes        | \"Just kidding\"\n",
    "John Cena |                 | Yes        | Similar use to \"Chuck Norris\"\n",
    "JOHN CENA |                 | Yes        | Comonly used spam phrase\n",
    "js        |                 | Yes        | \"Just sayin'\"\n",
    "k         | kk              | Yes        | \"Okay\"\n",
    "kappa     | keepo           | Yes        | Sarcasm\n",
    "kek       | kekekek         | Yes        | Translates to \"lol\"; laughter\n",
    "kms       |                 | Yes        | \"Kill myself\" (jokingly)\n",
    "kthx      | kthxbai         | Yes        | \"'kay-thanks(-bye)\"; implies firmness\n",
    "l8r       |                 | No         | \"Later\"\n",
    "leet      | l33t, 1337      | Yes        | Slang for \"elite\"\n",
    "lmao      | lmfao           | Yes        | \"Laughing my (fucking) ass off\"\n",
    "lmk       |                 | Rare       | \"Let me know\"\n",
    "lol       | lulz, lel, lawl | Yes        | \"Laugh out loud\"\n",
    "LPT       |                 | Yes        | \"Life Pro Tip\"\n",
    "lrl       |                 | Rare       | \"Laughing really loudly\"\n",
    "lrn2      |                 | Yes        | \"Learn to ...\"; e.g. \"lrn2read\"\n",
    "m8        | m9              | Yes        | \"Mate\"\n",
    "maga      |                 | Yes        | \"Make America Great Again\" (Trump)\n",
    "mfw       |                 | Yes        | \"My feeling when\"\n",
    "mrw       |                 | Yes        | \"My reaction when\"\n",
    "nerf      |                 | Yes        | \"Weaken\"; e.g. \"ak47 is op, plz nerf\"\n",
    "ngl       |                 | Yes        | \"Not gonna lie\"\n",
    "nm        |                 | Rare       | \"Not much\"; occasionally \"never mind\"\n",
    "nmu       |                 | Rare       | \"Not much, you?\"\n",
    "noob      | nub             | Rare       | Slang for \"newbie\"\n",
    "nu        |                 | Yes        | Cute \"no\"\n",
    "nvm       |                 | Yes        | \"Never mind\"\n",
    "ofc       |                 | Yes        | \"Of course\"\n",
    "omf       |                 | Rare       | \"Oh my fuck\"\n",
    "omg       | omfg            | Yes        | \"Oh my (fucking) god\"\n",
    "omw       |                 | Yes        | \"On my way\"\n",
    "ooc       |                 | Yes        | \"Out-of-context\"\n",
    "op        |                 | Yes        | \"Overpowered\"\n",
    "OP        |                 | Yes        | \"Original post(er)\"\n",
    "orly      |                 | Yes        | \"Oh really?\"\n",
    "pepe      |                 | Yes        | Poorly-drawn frog from 4chan\n",
    "pleb      | plebs           | Yes        | \"Plebian(s)\"; conformist; \"noob\"\n",
    "pleb tier |                 | Yes        | Of \"noob\"/conformist quality\n",
    "plz       | pls, pl0x       | Yes        | Please\n",
    "pron      | pr0n, pr0nz     | Yes        | \"Pornography\"\n",
    "pwned     | pwn             | Rare       | \"Powned\"; like \"owned\"\n",
    "REEEEEEEE |                 | Yes        | Onomatopoeia; very angry\n",
    "rekt      |                 | Yes        | \"Wrecked\"; like \"owned\"\n",
    "rickroll  |                 | Yes        | Common prank involving Rick Astley\n",
    "rip       |                 | Yes        | Laughter in response to failure\n",
    "rly       |                 | Yes        | \"Really\"\n",
    "rms       |                 | Rare       | [Richard Stallman](https://en.wikipedia.org/wiki/Richard_Stallman)\n",
    "rofl      |                 | Rare       | \"Rolling on floor laughing\"\n",
    "rotflol   |                 | No         | \"Rolling on floor laughing out loud\"\n",
    "rtfm      |                 | Yes        | \"Read the fucking manual\"\n",
    "rude      | rood            | Yes        | Response to insult/taunt\n",
    "shank     |                 | Yes        | Stab\n",
    "smd       |                 | Yes        | \"Suck my dick\"\n",
    "smh       |                 | Yes        | \"Shake my head\"\n",
    "soz       | sry             | Yes        | \"Sorry\"\n",
    "swag      | swagger         | Yes        | Coolness (sarcastic)\n",
    "tbf       |                 | Yes        | \"To be fair\"\n",
    "tbh       |                 | Yes        | \"To be honest\"\n",
    "tbt       |                 | Yes        | \"Throwback to\"\n",
    "TIFU      |                 | Yes        | \"Today I fucked up\" (e.g., \"TIFU by...\")\n",
    "tf        |                 | Yes        | \"... the fuck?\" / \"That feeling\"\n",
    "tfw       |                 | Yes        | \"That feeling when\"\n",
    "thx       |                 | Yes        | \"Thanks\"\n",
    "tide      |                 | Yes        | What angsty teens claim to eat\n",
    "TIL       |                 | Yes        | \"Today I learned\"\n",
    "tl;dr     | tl;dw           | Yes        | \"Too long; didn't read\" / \"... watch\"\n",
    "tmw       |                 | Yes        | \"That moment when\"\n",
    "tolo      |                 | Rare       | \"Tits out legs open\"\n",
    "topkek    | toplel          | Yes        | Neologism of \"kek\"; laughter\n",
    "ty        |                 | Yes        | \"Thank you\"\n",
    "uwotm8    |                 | Yes        | \"You what mate?!\"\n",
    "w00t      | woot            | No         | Exclamation of joy\n",
    "wb        |                 | Yes        | \"Welcome back\"\n",
    "wot       | wat             | Yes        | \"What\"; meant to sound stupid\n",
    "wtb       |                 | Yes        | \"Want-to-buy\"; looking to buy\n",
    "wtf       |                 | Yes        | \"What the fuck\"\n",
    "wtg       | w2g             | Rare       | \"Way to go\"; sarcastic\n",
    "wts       |                 | Yes        | \"Want-to-sell\"; looking to sell\n",
    "wuu2      |                 | Rare       | \"What [are] you up to?\"\n",
    "yarly     |                 | Yes        | \"Yeah, really\"\n",
    "ymmv      |                 | Yes        | \"Your mileage may vary\"\n",
    "yolo      | yoloswag        | Yes        | \"You only live once\"\n",
    "yw        |                 | Yes        | \"You're welcome\"\n",
    "\n",
    "## Cryptocurrency ##\n",
    "\n",
    "Slang     | Variations      | Meaning\n",
    ":---------|:----------------|:-------\n",
    "dump      |                 | Sell, often in panic\n",
    "hodl      |                 | \"Hold!\" (as opposed to \"sell\")\n",
    "moon      |                 | Where the value of cryptocurrency is headed\n",
    "pump      |                 | Buy, often to deliberately inflate price\n",
    "pumpdump  |                 | What whales do\n",
    "segwit    |                 | Segregated Witness (controversial update to BTC)\n",
    "segwit2x  |                 | Segregated Witness v2 (controversial update to BTC)\n",
    "altcoin   | shitcoin        | Anything other than bitcoin\n",
    "tether    |                 | A coin meant to be equal to 1 unit of fiat currency; used for regulation evasion\n",
    "whale     |                 | Rich person\n",
    "\n",
    "### Ticker symbols ###\n",
    "\n",
    "ID   | Currency       | Notes\n",
    ":--- |:-------------- |:-----\n",
    "BTC  | Bitcoin        | The original cryptocurrency\n",
    "BCH  | Bitcoin Cash   | Userbase is largely Chinese\n",
    "BTG  | Bitcoin Gold   |\n",
    "DOGE | Dogecoin       | Early joke altcoin\n",
    "ETH  | Ethereum       | Popular\n",
    "LTC  | Litecoin       | The original altcoin\n",
    "USDT | Tether: USD    |\n",
    "XBT  | Bitcoin        | Less common than BTC\n",
    "XRP  | Ripple         |\n",
    "\n",
    "\n",
    "# Emoticons #\n",
    "\n",
    "## Latin ##\n",
    "\n",
    "Emoticon   | Description\n",
    ":---------:|:-----------\n",
    ":)         | Smile\n",
    ":(         | Frown\n",
    ":P         | Raspberry (tongue sticking out)\n",
    ":c         | Cute frown\n",
    ":<         | Cute frown\n",
    "c:         | Cute smile\n",
    "<:         | Devious smile\n",
    ":L         | Uneasy; can also be drooling\n",
    ":l         | Uneasy; can also be drooling\n",
    ":/         | Uneasy or disapproving\n",
    ":\\\\        | Uneasy or disapproving\n",
    "^\\_^       | Cartoon smile\n",
    "^.^        | Cartoon smile\n",
    "\\>\\_\\<     | Cartoon grimace\n",
    "\\>.\\<      | Cartoon grimace\n",
    "\\>\\_\\>     | Looking sideways; avoiding eye contact\n",
    "\\<\\_\\<     | Looking sideways; avoiding eye contact\n",
    "\\>.\\>      | Looking sideways; avoiding eye contact\n",
    "\\<.\\<      | Looking sideways; avoiding eye contact\n",
    "-.-        | Angry stare (\"not amused\")\n",
    "-\\_-       | Angry stare (\"not amused\")\n",
    "o\\_o       | Small eyes; aghast\n",
    "o.o        | Small eyes; aghast\n",
    ".\\_.       | Small eyes; aghast\n",
    "owo        | Small eyes; aghast\n",
    "OwO        | Wide eyes; aghast\n",
    ";\\_;       | Crying\n",
    "\\>:)       | Evil grin\n",
    ":]         | Robot smile\n",
    ":}         | Devious smile\n",
    ":\\|        | Serious face\n",
    "\\>:(       | Angry\n",
    "\\>:\\|      | Angry\n",
    "-.^        | Raised eyebrow\n",
    "-\\_^       | Raised eyebrow\n",
    "8\\)        | Nerdy/geeky face\n",
    "B)         | Cool face with shades\n",
    "\\<3        | Heart\n",
    "xD         | Cringe-smile\n",
    ":3         | Cute, innocent kitty face\n",
    "x3         | Combination of xD and :3\n",
    "o/         | Waving\n",
    "\\\\o        | Waving\n",
    "\\\\o/       | Cheering; arms in the air\n",
    ";\\_;       | Crying\n",
    "OwO        | Cute, similar to :3 but newer, eyes open\n",
    "uwu        | Cute, similar to :3 but newer, eyes closed\n",
    "O:)        | Innocent; person with halo over head\n",
    ":#         | Mouth zippered shut\n",
    ":*         | Kiss (puckered lips)\n",
    ":v         | Cute, annoyed/obstinate; puffed-out cheek\n",
    "\n",
    "\n",
    "## Emoji ##\n",
    "\n",
    "Proper emoji use non-Latin characters to create complex, detailed faces.  There are a lot of combinations; I'll only list emoji that I see often.\n",
    "\n",
    "Emoji       | Description\n",
    ":----------:|:-----------\n",
    "¯\\\\\\_(ツ)\\_/¯  | Shrug (\"idk\")\n",
    "(╯°□°）╯︵ ┻━┻) | Flip table\n",
    "┬──┬ ¯\\\\\\_(ツ) | Restore flipped table\n",
    "(╯°□°）╯ | Gasp, hands in the air\n",
    "ಠ_ಠ | Skeptical/not-amused stare\n",
    "( ͡° ͜ʖ ͡°) | Trollface/\"Lenny\" (doesn't print properly in some browsers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = {\n",
    "'AFK': \"As far as I know\",\n",
    "'BTW':  \"By the way\",\n",
    "'GG' : \"Good Game\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As far as I know I am here'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion('AFK I am here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling Correction\n",
    "Library used for spelling correction:\n",
    "* NLTK\n",
    "* Spacy\n",
    "* Text Blob\n",
    "* PySpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certain conditions during several generations are modified in the same manner'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " incorrect_text = 'certan conditionas duriing seveal ggenrations aree moddified in the samee mannar'\n",
    " textblb=TextBlob(incorrect_text)\n",
    " textblb.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stop Words\n",
    "\n",
    "Stop words helps in formation of setence but not contribute to the meaning of sentences\n",
    "eg- a, the , of, are, my\n",
    "\n",
    "### NLTK Library has the list of Stopwords of English and other languages in nltk.corpus\n",
    "we do not remove the stop words if we have to do parts of speach tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reomve_stopwords(text):\n",
    "    new_text = []\n",
    "\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return ' '.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' name  Ankit. You    Great day'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reomve_stopwords(' my name is Ankit. You will have a Great day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Emojis\n",
    "options\n",
    " * Remove from the data\n",
    " * Replace with its meaning\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"Hi 🤔 How is your 🙈 and 😌. Have a nice weekend 💕👭👙\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi 🤔 How is your 🙈 and 😌. Have a nice weekend 💕👭👙'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi  How is your  and . Have a nice weekend '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace with a meaning\n",
    " with Library called emoji\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is your :see-no-evil_monkey:\n"
     ]
    }
   ],
   "source": [
    "print(emoji.demojize( \"How is your 🙈\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Process of breaking the text documents in to smaller part( words, sentances, phrases) called tokens.\n",
    "\n",
    "Q. Why TOkenization is needed?\n",
    "* finding no of unique words\n",
    "* Converting text to number\n",
    "\n",
    "Q. Problems in tokenization?\n",
    "* **Prefix:** characters at the beginning. eg- $20 =$ and 20\n",
    "* **Suffix:** Characters at the end. eg- 20kms = 20 and kms\n",
    "* **infix:**  Characters in between. eg- Hi! = Hi and !\n",
    "* **Exception:** Special-case rule to split a string into several tokens  or prevent a token from being split when punctuation rules are applied. eg- let's = let and us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1= 'I am an Indian'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization: Using Split function of python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "sent1= 'I am going to delhi'\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to delhi',\n",
       " ' I will stay there for 3 days',\n",
       " \" Let's hope the trip to be great\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence tokenization\n",
    "sent2 = 'I am going to delhi. I will stay there for 3 days. Let\\'s hope the trip to be great'\n",
    "sent2.split(\".\")    #spliting on dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitation in split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'Delhi!']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent3=  'I am going to Delhi!'\n",
    "sent3.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where do tthin I should go? I have 3 day holiday']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent4= \"Where do tthin I should go? I have 3 day holiday\"\n",
    "sent4.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokeniztion: using Regular Expression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'Delhi']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "tokens = re.findall(\"[\\w']+\",sent3)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem =\"\"\"A Bird Came Down.\n",
    "\n",
    "A Bird, came down the Walk -\n",
    "He did not know I saw\n",
    "He bit an Angle Worm in halves\n",
    "And ate the fellow, raw,\n",
    "\n",
    "And then, he drank a Dew\n",
    "From a convenient Grass -\n",
    "And then hopped sidewise to the Wall\n",
    "To let a Beetle pass -\n",
    "\n",
    "He glanced with rapid eyes,\n",
    "That hurried all abroad -\n",
    "They looked like frightened Beads, I thought,\n",
    "He stirred his Velvet Head. -\n",
    "\n",
    "Like one in danger, Cautious,\n",
    "I offered him a Crumb,\n",
    "And he unrolled his feathers,\n",
    "And rowed him softer Home -\n",
    "\n",
    "Than Oars divide the Ocean,\n",
    "Too silver for a seam,\n",
    "Or Butterflies, off Banks of Noon,\n",
    "Leap, plashless as they swim.\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Bird Came Down',\n",
       " '\\n\\nA Bird, came down the Walk -\\nHe did not know I saw\\nHe bit an Angle Worm in halves\\nAnd ate the fellow, raw,\\n\\nAnd then, he drank a Dew\\nFrom a convenient Grass -\\nAnd then hopped sidewise to the Wall\\nTo let a Beetle pass -\\n\\nHe glanced with rapid eyes,\\nThat hurried all abroad -\\nThey looked like frightened Beads, I thought,\\nHe stirred his Velvet Head',\n",
       " ' -\\n\\nLike one in danger, Cautious,\\nI offered him a Crumb,\\nAnd he unrolled his feathers,\\nAnd rowed him softer Home -\\n\\nThan Oars divide the Ocean,\\nToo silver for a seam,\\nOr Butterflies, off Banks of Noon,\\nLeap, plashless as they swim',\n",
       " '']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = re.compile('[.!?]').split(poem)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization using NLTK Library\n",
    "\n",
    "word tokenize and sentence tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to delhi.',\n",
       " 'I will stay there for 3 days.',\n",
       " \"Let's hope the trip to be great\"]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'going',\n",
       " 'to',\n",
       " 'delhi',\n",
       " '.',\n",
       " 'I',\n",
       " 'will',\n",
       " 'stay',\n",
       " 'there',\n",
       " 'for',\n",
       " '3',\n",
       " 'days',\n",
       " '.',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'hope',\n",
       " 'the',\n",
       " 'trip',\n",
       " 'to',\n",
       " 'be',\n",
       " 'great']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'Ph.D', 'in', 'A.I']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problems in nltk\n",
    "sent5= 'I have Ph.D in A.I'\n",
    "sent6= \"we're here to help! mail us at akt@gmail.com\"\n",
    "sent7= 'A 5km race cost us $10'\n",
    "\n",
    "word_tokenize(sent5) # all correctly tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " \"'re\",\n",
       " 'here',\n",
       " 'to',\n",
       " 'help',\n",
       " '!',\n",
       " 'mail',\n",
       " 'us',\n",
       " 'at',\n",
       " 'akt',\n",
       " '@',\n",
       " 'gmail.com']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent6)\n",
    "# wrongly toknize-  'akt', '@', 'gmail.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '5km', 'race', 'cost', 'us', '$', '10']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent7)\n",
    "# 5km is wrongly tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization using Spacy Library - best for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp= spacy.load('en_core_web_sm') # loading english small dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting all sentences to documents\n",
    "doc1 = nlp(sent5)\n",
    "doc2 = nlp(sent6)\n",
    "doc3 = nlp(sent7)\n",
    "doc4 = nlp(sent1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I have Ph.D in A.I"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "Ph\n",
      ".\n",
      "D\n",
      "in\n",
      "A.I\n"
     ]
    }
   ],
   "source": [
    "for  token in doc1:\n",
    "    print(token)\n",
    "# failed in Ph.D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we\n",
      "'re\n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "mail\n",
      "us\n",
      "at\n",
      "akt@gmail.com\n"
     ]
    }
   ],
   "source": [
    "for  token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "race\n",
      "cost\n",
      "us\n",
      "$\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for  token in doc3:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "going\n",
      "to\n",
      "delhi\n"
     ]
    }
   ],
   "source": [
    "for  token in doc4:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmimg\n",
    "- used in information retrieval system\n",
    "\n",
    "In Grammar, Inflection is the modification of a word to express different grammatical categorical such as tense, case, voice, aspect, person, number, gender and mood\n",
    "\n",
    "eg- walk=> walking, walks, walker, walked\n",
    "eg- do-> undoable, done, does, did\n",
    "\n",
    "\n",
    "### NLTK Library is used which has multiple stemmers\n",
    "stemmers- an algorithm that do stemmimg made by language expert aka- Linguistic experts\n",
    "Porter stemmer is one of the famous stemmer for english language\n",
    "snowball stemmer is used for other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    stemmed_words = [ps.stem(word) for word in text.split()]   \n",
    "    return ' '.join(stemmed_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 'walk walks walking walked'\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''He walked down the steps from the train station in a bit of a hurry knowing the secrets in the briefcase must be secured as quickly as possible. Bounding down the steps, he heard something behind him and quickly turned in a panic. There was nobody there but a pair of old worn-out shoes were placed neatly on the steps he had just come down. Had he past them without seeing them? It didn't seem possible. He was about to turn and be on his way when a deep chill filled his body.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"he walk down the step from the train station in a bit of a hurri know the secret in the briefcas must be secur as quickli as possible. bound down the steps, he heard someth behind him and quickli turn in a panic. there wa nobodi there but a pair of old worn-out shoe were place neatli on the step he had just come down. had he past them without see them? it didn't seem possible. he wa about to turn and be on hi way when a deep chill fill hi body.\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemetization\n",
    "Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called **Lemma**. A Lemma(plural lemmas or lemmata) is the canonical form, or citation form of a set of words\n",
    "\n",
    "### Difference between Stemming and Lemetization\n",
    "- whenever we have to show the output to someone we use Lemitization as the output of stemming may not be the word of english but output of lemitization is always word of english.\n",
    "\n",
    "- Stemmimg works on algorithms eg- PorterStemmer. Lemitization refers Dictionary. eg- WordNetLemmatizer(lexical dictionary) \n",
    "\n",
    "- Stemmimg is faster than Lemitization (as algorithms are faster as compare to searching in the dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '''He walked down the steps from the train station in a bit of a hurry knowing the secrets \n",
    "in the briefcase must be secured as quickly as possible. Bounding down the steps, he heard something \n",
    "behind him and quickly turned in a panic. There was nobody there but a pair of old worn-out shoes were \n",
    "placed neatly on the steps he had just come down. Had he past them without seeing them? It didn't seem \n",
    "possible. He was about to turn and be on his way when a deep chill filled his body.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = \"?:!,.;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word                \n",
      "He                  He                  \n",
      "walked              walk                \n",
      "down                down                \n",
      "the                 the                 \n",
      "steps               step                \n",
      "from                from                \n",
      "the                 the                 \n",
      "train               train               \n",
      "station             station             \n",
      "in                  in                  \n",
      "a                   a                   \n",
      "bit                 bite                \n",
      "of                  of                  \n",
      "a                   a                   \n",
      "hurry               hurry               \n",
      "knowing             know                \n",
      "the                 the                 \n",
      "secrets             secrets             \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "briefcase           briefcase           \n",
      "must                must                \n",
      "be                  be                  \n",
      "secured             secure              \n",
      "as                  as                  \n",
      "quickly             quickly             \n",
      "as                  as                  \n",
      "possible            possible            \n",
      "Bounding            Bounding            \n",
      "down                down                \n",
      "the                 the                 \n",
      "steps               step                \n",
      "he                  he                  \n",
      "heard               hear                \n",
      "something           something           \n",
      "behind              behind              \n",
      "him                 him                 \n",
      "and                 and                 \n",
      "quickly             quickly             \n",
      "turned              turn                \n",
      "in                  in                  \n",
      "a                   a                   \n",
      "panic               panic               \n",
      "There               There               \n",
      "was                 be                  \n",
      "nobody              nobody              \n",
      "there               there               \n",
      "but                 but                 \n",
      "a                   a                   \n",
      "pair                pair                \n",
      "of                  of                  \n",
      "old                 old                 \n",
      "worn-out            worn-out            \n",
      "shoes               shoe                \n",
      "were                be                  \n",
      "placed              place               \n",
      "neatly              neatly              \n",
      "on                  on                  \n",
      "the                 the                 \n",
      "steps               step                \n",
      "he                  he                  \n",
      "had                 have                \n",
      "just                just                \n",
      "come                come                \n",
      "down                down                \n",
      "Had                 Had                 \n",
      "he                  he                  \n",
      "past                past                \n",
      "them                them                \n",
      "without             without             \n",
      "seeing              see                 \n",
      "them                them                \n",
      "It                  It                  \n",
      "did                 do                  \n",
      "n't                 n't                 \n",
      "seem                seem                \n",
      "possible            possible            \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "about               about               \n",
      "to                  to                  \n",
      "turn                turn                \n",
      "and                 and                 \n",
      "be                  be                  \n",
      "on                  on                  \n",
      "his                 his                 \n",
      "way                 way                 \n",
      "when                when                \n",
      "a                   a                   \n",
      "deep                deep                \n",
      "chill               chill               \n",
      "filled              fill                \n",
      "his                 his                 \n",
      "body                body                \n"
     ]
    }
   ],
   "source": [
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "sentence_words\n",
    "\n",
    "print(\"{0:20}\".format('word','Lemma'))\n",
    "for word in sentence_words:\n",
    "    print(\"{0:20}{1:20}\".format(word, wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\acz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\acz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\acz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\acz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\acz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\acz\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acz\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Set the download directory (replace 'your/download/directory' with the desired path)\n",
    "nltk.data.path.append('your/download/directory')\n",
    "\n",
    "# Download the WordNet resource\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
